---
title: "IST - Lab 05: Serverless data ingestion and processing"
author: "Farouk Ferchichi & Hugo Huart"
format:
    pdf:
        geometry:
            margin=1.5cm
---

# IST - LAB 05 : Serverless data ingestion and processing

**Farouk Ferchichi & Hugo Huart**

# Task 1: Explore MeteoSwiss data

## _For the two data products copy the URLs where the data can be downloaded in the report_

For "Automatic weather stations – Current measurement values" the URL is 
https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv

For "Weather stations of the automatic monitoring network" the URL is
https://data.geo.admin.ch/ch.meteoschweiz.messnetz-automatisch/ch.meteoschweiz.messnetz-automatisch_en.json

## _Document your exploration of the measurement values_

### Nearby weather station

The closest weather station to where I live is the Vevey/Corseaux station. Its 3 letters ID is `VEV`.

### Temperature column

According to the data description provided at the URL https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/info/VQHA80_en.txt,
the column `tre200s0` corresponds to the temperature in °C 2 meters above ground

### Current temperature

The temperature is 9.9°C.

### Comparison with MétéoSuisse

Yes, it corresponds with the temperature shown by MétéoSuisse on the app.

### Date column

It contains the date and time of the measurement with format YYYYMMDDHHmm, in UTC time (not CET).
The minutes are a multiple of 10 minutes (00, 10, 20, etc.)

# Task 2: Upload the current measurement data to S3 and run SQL queries on it

A bucket was created with name `ist-meteo-grh-ferchichi-huart`.

A database with name `meteoswiss_grh` was created.

The `current` table was created, with the additional SerDe option `skip.header.line.count` set to 1 to prevent
the inclusion of original the header in the table.

# Task 3: Write a python script to download the current measurement values from MeteoSwiss and upload them to S3

Here's the Python script used:

```python
import boto3
import requests

BUCKET="ist-meteo-grh-ferchichi-huart"
KEY="current/VQHA80.csv"

CSV_URL = "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv"

def fetch_csv(url:str=CSV_URL) -> bytes:
    return requests.get(url).content

def upload_to_s3(csv_data:bytes, bucket_name:str, key:str):
    s3 = boto3.resource("s3")
    obj = s3.Object(bucket_name, key)
    obj.put(Body=csv_data)

def main():
    csv_data = fetch_csv()
    upload_to_s3(csv_data, BUCKET, KEY)

if __name__ == "__main__":
    main()
```

The script was tested successfully

# Task 4: Convert your script into an AWS Lambda function for data ingestion

The following entities were created:

* A Lambda function named `meteoswiss-ingest-grh`
* An IAM policy named `meteoswiss-grh-write` with `PutObject` permission on the S3 bucket.

The policy was assigned to the Lambda role. Then an .zip upload package was created with the Lambda handler script and the `requests` package.

Here's the handler script code:

```python
import os
import datetime
import boto3
import json
import requests

BUCKET="ist-meteo-grh-ferchichi-huart"
KEY="current/VQHA80.csv"

CSV_URL = "https://data.geo.admin.ch/ch.meteoschweiz.messwerte-aktuell/VQHA80.csv"

def fetch_csv(url:str=CSV_URL) -> bytes:
    return requests.get(url).content

def upload_to_s3(csv_data:bytes, bucket_name:str, key:str):
    s3 = boto3.resource("s3")
    obj = s3.Object(bucket_name, key)
    obj.put(Body=csv_data)

def timestamp_key(key:str) -> str:
    now = datetime.datetime.utcnow().isoformat()
    key = os.path.splitext(key)[0]
    return f"{key}-{now}.csv"
    
def main():
    csv_data = fetch_csv()
    key = timestamp_key(KEY)
    upload_to_s3(csv_data, BUCKET, key)

def lambda_handler(event, context):
    main()
    return {
        'statusCode': 200,
        'body': json.dumps('')
    }
```

It was successfully tested.

# Task 5: Create an event rule that triggers your function every 10 minutes

A 10-minutes schedule named `MeteoswissIngestGrHFerchichiHuart` was created.

# Task 6: transform the weather stations file into a csv file

## _Examine the YAML. There are two top-level keys, what are their names?_

There are several top level keys, which are `crs`, `license`, `mapname`, `map_long_name`, `map_short_name`, `map_abstract`, `creation_time`,
`type` and finally `features` which is an array.

## _What key contains the station name?_

The `station_name` keys

## Final jq command

Here's the final JQ command:

```bash
jq -j '.features|.[]|.id,",\"",.properties.station_name,"\",",.properties.altitude,",",.geometry.coordinates[0],",",.geometry.coordinates[1],"\n"'
```

# Task 7: Query the accumulated data

## All Payerne measurements

```sql
SELECT * FROM "meteoswiss_grh"."current"
WHERE station = 'PAY'
ORDER BY datetime ASC
```

## Hourly Payerne measurements

```sql

```